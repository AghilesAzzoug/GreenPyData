<p>It is recommended to use DistributedDataParallel instead of DataParallel to do multi-GPU training, <b>even if there
    is only a single node</b>. See PyTorch documentation for <a
        href="https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html#torch.nn.parallel.DistributedDataParallel">nn.parallel.DistributedDataParallel</a>
    instead of multiprocessing or nn.DataParallel and Distributed Data Parallel..</p>
<p>Don't forget to call init_process_group to initialize the default distributed process group <b>(even if there is
    only a single node)</b>. See PyTorch <a
        href="https://pytorch.org/tutorials/intermediate/ddp_tutorial.html#basic-use-case">example</a>.</p>
<h2>Noncompliant Code Example</h2>
<pre>
    import torch
    my_model = ...
    my_model = torch.nn.DataParallel(my_model)
    ...
</pre>
<h2>Compliant Solution</h2>
<pre>
    import torch
    my_model = ...
    setup_world(rank, world_size)
    my_model = torch.nn.parallel.DistributedDataParallel(my_model)
    ...
</pre>
